{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datsets : \n",
    "1. Wikipidea dataset : ```https://huggingface.co/datasets/legacy-datasets/wikipedia```\n",
    "2. BookCoupus Dataset : ```http://huggingface.co/datasets/bookcorpus/bookcorpus```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to be done in the notebook\n",
    " 1. Train Tokenizer\n",
    " 2. Train RoBERTa\n",
    " 3. Finetune RoBERTa for MLM & QA\n",
    " 4. Get Accuracy metrix for MLM & QA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train Tokenizer\n",
    "\n",
    "We are training the tokenizer on book-corpus dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pankaj Deb Roy\\Documents\\DeepLearning\\.env_llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer,ByteLevelBPETokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from transformers import GPT2TokenizerFast,PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<unk>\"\n",
    "bos_tok = \"<sos>\"\n",
    "eos_tok = \"<eos>\"\n",
    "pad_tok = \"<pad>\"\n",
    "mask_tok = \"<mask>\"\n",
    "special_tokens = [unk_token,bos_tok,eos_tok,pad_tok,mask_tok]\n",
    "\n",
    "def prepare_sentencepiece_training(alg):\n",
    "    # tokenizer = SentencePieceBPETokenizer(unk_token=unk_token)\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(model=BPE(unk_token=unk_token))\n",
    "        trainer = BpeTrainer(\n",
    "            special_tokens = special_tokens,\n",
    "            vocab_size=30000,\n",
    "            min_frequency=10,\n",
    "            show_progress=True,\n",
    "            max_token_length=5,\n",
    "            )\n",
    "    elif alg=='ByteLevelBPE':\n",
    "        tokenizer = ByteLevelBPETokenizer()\n",
    "        trainer =None\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    \n",
    "    return tokenizer,trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(files,tokenizer_folder = \"./tokenizer/\",alg=\"ByteLevelBPE\",type=None):\n",
    "    os.makedirs(tokenizer_folder,exist_ok=True)\n",
    "\n",
    "    tokenizer,trainer = prepare_sentencepiece_training('ByteLevelBPE')\n",
    "\n",
    "    if alg=='BPE':\n",
    "        tokenizer.train(\n",
    "        files,\n",
    "        trainer\n",
    "        )\n",
    "        tokenizer.save(os.path.join(tokenizer_folder,'vocab.json'))\n",
    "        print(f\"Tokenizer Type {\"Tokenizer\"}\\nTokenizer Saved to {tokenizer_folder}\")\n",
    "        print(f\"from tokenizers import Tokenizer\\ntokenizer = Tokenizer.from_file('{os.path.join(tokenizer_folder,'vocab.json')}')\")\n",
    "\n",
    "\n",
    "    elif alg=='ByteLevelBPE':\n",
    "        tokenizer.train(\n",
    "            files,\n",
    "            special_tokens = special_tokens,\n",
    "            vocab_size=30000,\n",
    "            min_frequency=10,\n",
    "            show_progress=True,\n",
    "        )\n",
    "        tokenizer.save_model(tokenizer_folder)\n",
    "        if type==None:\n",
    "            transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "                    tokenizer_object=tokenizer,\n",
    "                    pad_token=pad_tok,\n",
    "                    bos_token=bos_tok,\n",
    "                    eos_token=eos_tok,\n",
    "                    unk_token=unk_token,\n",
    "                    mask_tok=mask_tok,\n",
    "                    padding_side=\"right\",\n",
    "                    clean_up_tokenization_spaces=False,\n",
    "                    )\n",
    "\n",
    "        elif type=='GPT2':\n",
    "            transformer_tokenizer = GPT2TokenizerFast.from_pretrained(tokenizer_folder,\n",
    "                                                                    pad_token=pad_tok,\n",
    "                                                                    bos_token=bos_tok,\n",
    "                                                                    eos_token=eos_tok,\n",
    "                                                                    unk_token=unk_token,\n",
    "                                                                    mask_tok=mask_tok,\n",
    "                                                                    )\n",
    "        transformer_tokenizer.save_pretrained(tokenizer_folder)\n",
    "        print(f\"Tokenizer Type '{\"PreTrainedTokenizerFast\" if type==None else \"GPT2TokenizerFast\"}'\\nTokenizer Saved to {tokenizer_folder}\")\n",
    "        print(f\"from transformers import GPT2TokenizerFast,PreTrainedTokenizerFast\\ntokenizer = {\"PreTrainedTokenizerFast\" if type==None else \"GPT2TokenizerFast\"}.from_pretrained('{tokenizer_folder}')\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Type PreTrainedTokenizerFast\n",
      "Tokenizer Saved to tokenizer\n",
      "from transformers import GPT2TokenizerFast,PreTrainedTokenizerFast\n",
      "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer)\n"
     ]
    }
   ],
   "source": [
    "#Start Training\n",
    "PTH = \"C:\\\\Users\\\\Pankaj Deb Roy\\\\Documents\\\\DeepLearning\\\\Dataset\\\\bookcopous\\\\bookcorpus\"\n",
    "files = [os.path.join(PTH,file) for file in os.listdir(PTH)]\n",
    "train_tokenizer(files=files,tokenizer_folder=\"tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pankaj Deb Roy\\Documents\\DeepLearning\\.env_llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import csv\n",
    "import codecs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import GPT2TokenizerFast,PreTrainedTokenizerFast,RobertaTokenizerFast\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import random\n",
    "import tqdm\n",
    "\n",
    "from mlm_dataset import MLMDataset,MLMDatasetHF\n",
    "from trainer import TrainingArgs,Trainer\n",
    "\n",
    "#GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "# torch.cuda.set_device(0)\n",
    "# torch.cuda.current_device()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device ',device)\n",
    "#CPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\",\"\n",
    "# torch.cpu.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext_dataset_pth=\"C:\\\\Users\\\\Pankaj Deb Roy\\\\Documents\\\\DeepLearning\\\\Dataset\\\\wikitext-103-raw-v1\"\n",
    "train_files = [file for file in os.listdir(wikitext_dataset_pth) if file.startswith('train')]\n",
    "validataion_files = [file for file in os.listdir(wikitext_dataset_pth) if file.startswith('validation')]\n",
    "test_files = [file for file in os.listdir(wikitext_dataset_pth) if file.startswith('test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(files,csv_name):\n",
    "    ENCODING = \"utf-8\" \n",
    "    CSV_NAME = os.path.join(wikitext_dataset_pth,csv_name)\n",
    "    id = 0\n",
    "    with codecs.open(CSV_NAME,  \"w\", ENCODING) as csvfile:\n",
    "        CSVWriter = csv.writer(csvfile, quoting=csv.QUOTE_MINIMAL)\n",
    "        CSVWriter.writerow(['text'])\n",
    "        for file in files:\n",
    "            parquet_file = pq.ParquetFile(os.path.join(wikitext_dataset_pth,file))\n",
    "            for i in parquet_file.iter_batches(batch_size=1):\n",
    "                \n",
    "                data = i.to_pandas().iloc[0].text.split('\\n')\n",
    "                for line in data:\n",
    "                    if len(line)<30 or line.startswith('= ='):\n",
    "                        pass\n",
    "                    else:\n",
    "                        id+=1\n",
    "                        CSVWriter.writerow([line])\n",
    "    print(f'Total number of rows in {csv_name}: {id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in train_wiki-104.csv: 823942\n",
      "Total number of rows in val_wiki-104.csv: 1777\n",
      "Total number of rows in test_wiki-104.csv: 2007\n"
     ]
    }
   ],
   "source": [
    "create_dataframe(train_files,'train_wiki-104.csv')\n",
    "create_dataframe(validataion_files,'val_wiki-104.csv')\n",
    "create_dataframe(test_files,'test_wiki-104.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"./mlm_roberta/base_model_e900.pt\",map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:05<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange, tqdm\n",
    "from time import sleep\n",
    "for i in trange(5,10):\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
